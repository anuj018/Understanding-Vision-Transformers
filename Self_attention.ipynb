{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a789e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bfde633",
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_of_imput_sequence, d_k,d_v = 4,8,8\n",
    "Q = np.random.randn(Length_of_imput_sequence,d_k)\n",
    "K = np.random.randn(Length_of_imput_sequence,d_k)\n",
    "V = np.random.randn(Length_of_imput_sequence,d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e848a44",
   "metadata": {},
   "source": [
    "Self attention = softmax((dot(Q,K)/math.sqrt(d_k)) + M)V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79447359",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_matmul_K = np.matmul(Q,K.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f8c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Q_matmul_K = Q_matmul_K/math.sqrt(d_k) # reduces variance of the Q_matmul_K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a48478",
   "metadata": {},
   "source": [
    "## Masking\n",
    "Masking is needed in the decoder section to not look at the future word when creating the context of the current word\n",
    "However, Masks can also be used in the encoder section such that the each input token is prevented from getting contect from the tokens that appear later in the sequence\n",
    "\n",
    "Masking can be emulated by simply using a lower triangular matrix whose values below the diagonal are all 0's and above them is -inf. The reason why we use 0 is because we add this mask to the scaled matmul(Q,K.T) and thereby not affecting its values and -inf to enable softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b3905fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.tril(np.ones((Length_of_imput_sequence,Length_of_imput_sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc87b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d35f5e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee1a3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Masked_Scaled_Q_matmul_K = (Scaled_Q_matmul_K + mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc345cdf",
   "metadata": {},
   "source": [
    " ## Softmax\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "262a3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Masked_Scaled_Q_matmul_K):\n",
    "    return (np.exp(Masked_Scaled_Q_matmul_K).T/np.sum(np.exp(Masked_Scaled_Q_matmul_K),axis = 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "512c0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(Masked_Scaled_Q_matmul_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a01e385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.6271699 , 0.3728301 , 0.        , 0.        ],\n",
       "       [0.19705597, 0.57663204, 0.22631199, 0.        ],\n",
       "       [0.60900332, 0.07995933, 0.02305373, 0.28798363]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1faf5",
   "metadata": {},
   "source": [
    "Notice the effect of using a mask here ^ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cd810f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_v = np.matmul(attention,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7169308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83241343,  0.65783222,  1.02220733, -0.4018901 , -0.24618614,\n",
       "         0.28513707,  0.9904864 , -0.75213917],\n",
       "       [ 0.2289083 ,  0.35576851,  0.63442191, -0.36325503, -0.70928762,\n",
       "         0.03312295,  0.81402309, -1.15149977],\n",
       "       [-0.10371799,  0.09063954,  0.32839326, -0.04086265, -1.28331618,\n",
       "        -0.43473941,  0.47172228, -1.24125348],\n",
       "       [ 0.5463514 ,  0.32616912, -0.04976875, -0.07038777,  0.25460342,\n",
       "         0.2552575 ,  0.98547674, -0.23249863]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c13e3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83241343,  0.65783222,  1.02220733, -0.4018901 , -0.24618614,\n",
       "         0.28513707,  0.9904864 , -0.75213917],\n",
       "       [-0.7863001 , -0.15235909, -0.01790564, -0.29826362, -1.48831083,\n",
       "        -0.39081189,  0.51717882, -1.82329893],\n",
       "       [ 0.82035336,  0.21591835,  0.60662348,  0.92933901, -1.66405686,\n",
       "        -1.17348054, -0.09580076, -0.18411479],\n",
       "       [ 0.28949104, -0.23351355, -2.37808524,  0.61388511,  1.9511479 ,\n",
       "         0.48582706,  1.19146543,  1.30420922]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8136a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(K,Q,V,mask = None):\n",
    "    d_k = K.shape[-1]\n",
    "    scaled = np.matmul(Q,K.T)/math.sqrt(d_k)\n",
    "    scaled = scaled if mask is None else scaled+mask\n",
    "    attention = softmax(scaled)\n",
    "    new_v = np.matmul(attention,V)\n",
    "    return new_v , attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "554e7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values,attention = scaled_dot_product_attention(K,Q,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "365fc108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18124485,  0.155739  ,  0.03281338,  0.0279723 , -0.55665541,\n",
       "        -0.16453249,  0.67447618, -0.68784406],\n",
       "       [ 0.35107052,  0.13416902, -0.38009518,  0.18686706,  0.01800877,\n",
       "        -0.02293615,  0.78384793, -0.16179826],\n",
       "       [-0.06313566,  0.05718434,  0.04906295,  0.02671258, -0.94949352,\n",
       "        -0.33972956,  0.54600555, -0.97854129],\n",
       "       [ 0.5463514 ,  0.32616912, -0.04976875, -0.07038777,  0.25460342,\n",
       "         0.2552575 ,  0.98547674, -0.23249863]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5621b",
   "metadata": {},
   "source": [
    "# MultiHeaded Attention\n",
    "\n",
    "breaking down each word vector into N different parts and multiplyting each of them with different Linear Layers with different weights to get a better contextual understanding of the sequence of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278dbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
